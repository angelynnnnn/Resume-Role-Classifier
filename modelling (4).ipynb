{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "501cb99f",
      "metadata": {
        "id": "501cb99f"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96f16a60",
      "metadata": {
        "id": "96f16a60"
      },
      "source": [
        "### Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "167724f4",
      "metadata": {
        "id": "167724f4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import KFold\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db61c7e7",
      "metadata": {
        "id": "db61c7e7"
      },
      "source": [
        "### Importing cleaned dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba471d78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563
        },
        "id": "ba471d78",
        "outputId": "cbed1b9e-cf1c-43f9-a704-e9c87b25340d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Category                                             Resume  \\\n",
              "0      Accountant  education omba executive leadership university...   \n",
              "1      Accountant  howard gerrard accountant deyjobcom birmingham...   \n",
              "2      Accountant  kevin frank senior accountant inforesumekraftc...   \n",
              "3      Accountant  place birth nationality olivia ogilvy accounta...   \n",
              "4      Accountant  stephen greet cpa senior accountant 9 year exp...   \n",
              "...           ...                                                ...   \n",
              "12239     Testing  Computer Skills: â¢ Proficient in MS office (...   \n",
              "12240     Testing  â Willingness to accept the challenges. â ...   \n",
              "12241     Testing  PERSONAL SKILLS â¢ Quick learner, â¢ Eagerne...   \n",
              "12242     Testing  COMPUTER SKILLS & SOFTWARE KNOWLEDGE MS-Power ...   \n",
              "12243     Testing  Skill Set OS Windows XP/7/8/8.1/10 Database MY...   \n",
              "\n",
              "                                              clean_text  \n",
              "0      education omba executive leadership  bachelor ...  \n",
              "1       accountant deyjobcom  infodayjobcom linkedinn...  \n",
              "2       senior accountant inforesumekraftcom chicago ...  \n",
              "3      place birth nationality olivia  accountant 151...  \n",
              "4       cpa senior accountant year experience establi...  \n",
              "...                                                  ...  \n",
              "12239   in MS office Word Basic Excel Power point  wo...  \n",
              "12240  Willingness to accept the challenges Positive ...  \n",
              "12241  PERSONAL SKILLS Quick learner Eagerness to lea...  \n",
              "12242  COMPUTER SKILLS SOFTWARE KNOWLEDGE  wind  I nt...  \n",
              "12243   Windows XP Database MYSQL sql server 2005 200...  \n",
              "\n",
              "[12244 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7a28e0b1-625a-468a-a9d2-1678d8b4f97e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Resume</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>education omba executive leadership university...</td>\n",
              "      <td>education omba executive leadership  bachelor ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>howard gerrard accountant deyjobcom birmingham...</td>\n",
              "      <td>accountant deyjobcom  infodayjobcom linkedinn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>kevin frank senior accountant inforesumekraftc...</td>\n",
              "      <td>senior accountant inforesumekraftcom chicago ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>place birth nationality olivia ogilvy accounta...</td>\n",
              "      <td>place birth nationality olivia  accountant 151...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Accountant</td>\n",
              "      <td>stephen greet cpa senior accountant 9 year exp...</td>\n",
              "      <td>cpa senior accountant year experience establi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12239</th>\n",
              "      <td>Testing</td>\n",
              "      <td>Computer Skills: â¢ Proficient in MS office (...</td>\n",
              "      <td>in MS office Word Basic Excel Power point  wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12240</th>\n",
              "      <td>Testing</td>\n",
              "      <td>â Willingness to accept the challenges. â ...</td>\n",
              "      <td>Willingness to accept the challenges Positive ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12241</th>\n",
              "      <td>Testing</td>\n",
              "      <td>PERSONAL SKILLS â¢ Quick learner, â¢ Eagerne...</td>\n",
              "      <td>PERSONAL SKILLS Quick learner Eagerness to lea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12242</th>\n",
              "      <td>Testing</td>\n",
              "      <td>COMPUTER SKILLS &amp; SOFTWARE KNOWLEDGE MS-Power ...</td>\n",
              "      <td>COMPUTER SKILLS SOFTWARE KNOWLEDGE  wind  I nt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12243</th>\n",
              "      <td>Testing</td>\n",
              "      <td>Skill Set OS Windows XP/7/8/8.1/10 Database MY...</td>\n",
              "      <td>Windows XP Database MYSQL sql server 2005 200...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12244 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7a28e0b1-625a-468a-a9d2-1678d8b4f97e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7a28e0b1-625a-468a-a9d2-1678d8b4f97e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7a28e0b1-625a-468a-a9d2-1678d8b4f97e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b1b35a16-1f13-4390-aa31-baf814c937d1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b1b35a16-1f13-4390-aa31-baf814c937d1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b1b35a16-1f13-4390-aa31-baf814c937d1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_da21943a-02f7-4714-b8d0-c64125a45e27\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_da21943a-02f7-4714-b8d0-c64125a45e27 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12244,\n  \"fields\": [\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 43,\n        \"samples\": [\n          \"React Developer\",\n          \"Finance\",\n          \"Food and Beverages\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Resume\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12244,\n        \"samples\": [\n          \"summary jessica claire montgomery street san francisco ca 94105 555 4321000 resumesampleexamplecom individual reliable strong minded hard working handles multiple responsibilities providing excellent customer service motivation learn new skills improve quality work expand knowledge prior court coordinator honorable judge barbara e roberts looking challenging career path allow build upon diversified background allows opportunity advance within galveston county skills proven ability team player strong interpersonal skills ability research identify problem provide input finding solution 6 years customer service knowledge county clerks office policies procedures knowledge district courts policies procedures odyssey software program microsoft office organization strong work ethic experience case management lifepoint hospitals bryson city nc 012019 current purpose purpose case management provide outline possible duties responsibilities original court coordinator district courts primary duties primary duties position manage direct supervise coordinate plan operations courts assist judiciary making certain decisions absence original court coordinator except judicial decisions required law made judges description covers duties involve criminal cases civil cases family cases specific duties maintain control master docket courts including setting preparation weekly andor monthly dockets court maintain close contact district attorneys office allow sufficient docket control criminal docket attend assist courts calling dockets get announcements resetting cases date purpose certain supervise notification attorneys case settings supervise preparation duplication distribution dockets use court personnel initiate better efficient procedures dockets caseflow enable fair speedy disposition court cases maintain data records caseflow time elements involved cases planning performance purposes inform court bailiff defendants custody scheduled dates times appearances court check attorneys status cases set trial keep courts informed changes status weekly monthly dockets prepare nonroutine correspondence judge assist administrative duties requested required judges assisting daily emails andor phone calls administration development review submit vouchers completed attorneys handle female drug testing requested judge prepare vacation letters completed attorneys coort coordinator choctaw nation oklahoma pocola ok purpose 032017 012019 purpose position provide outline possible duties responsibilities court coordinator county court law primary duties primary duties position manage direct supervise coordinate plan operations courts assist judiciary making certain decisions except judicial decisions required law made judges specific duties docket management maintain control master docket courts including setting preparation weekly monthly dockets court maintain close contact district attorneys office allow sufficient docket control criminal docket attend assist courts calling dockets get announcements resetting cases date purpose certain supervise notification attorneys case settings supervise preparation duplication distribution dockets use court personnel initiate better efficient procedures dockets caseflow enable fair speedy disposition court cases maintain data records caseflow time elements involved cases planning performance purposes utilize electronic case management systems order track cases determine time limits set local rules met inform court bailiff defendants custody scheduled dates times appearances court check attorneys status cases set trial keep courts informed changes status weekly monthly dockets trial management set trial dockets plan supervise mechanics notice connected trials coordinate availability attorneys parties court personnel reset cases reached trial determine coordinate availability visiting judges regional presiding judge plan order proper time summons jurors adequate courts keeping mind economy efficiency based courts schedule docket send jury thank letters questionnaires trial personnel management research report personnel policies required court records reports supervise record keeping court functions dockets prepare periodic reports requested judges courts departments prepare annual report court work cases processed maintain schedules statistics matters request judges court general public professional development public relations reply inquiries public court business maintain special legal resource files obtain legal materials requested judge general administration expedite nonroutine administrative matters arise prepare recommend rule changes prepare nonroutine correspondence judge assist administrative duties requested required judges deouty court clerk civil misdemeanor 102013 032017 galveton county clerks office city state purpose purpose court clerk provide support overseeing administrative duties related courts function specific duties answers directs incoming calls appropriate county departments andor individuals provides customer service information public outside agencies front counter telephone fax andor mail responds inquiries regarding individual cases judgments court processes procedures reviews related documentation accuracy completeness processes public records requests researches retrieves case records provides files public viewing andor prepares copies requested legal documents calculates copy fees locates provides legal documents requested law enforcement personnel assists attorneys preparing pulling case files schedules court dates prepares docket notices andor types court documents processes warrants restraining orders judgments issues writs garnishment andor restitution performs variety records management functions organizes files scans indexes copies retrieves certifies court records documents coordinates destruction documents assigned attends records court hearings assigned performs clerical duties departmental staff assigned required daocare worker star kids daycare learning center city state purpose 052009 052013 daycare workers ensure safety welfare children school times parents working specific duties providing care children setting schedules routines grooming feeding changing diapers cleaning rooms toys developing encouraging ageappropriate learning socialization ensure children learn basic skills concepts communication manners sharing etc maintaining safe workplace monitoring children health behavioral emotional issues reporting concerns staff parents helping children discover new interests introducing art music sports potential hobbies ensuring children learning positive behaviors providing guidance approved discipline needed preparing children enter next level care entry school keeping records relating child care working parents help children progress towards educational behavioral goals education certificate completion law enforcement academy 052013 training galveston college galveston tx high school diploma texas city high school texas city tx 052009\",\n          \"jessica claire resumesampleexamplecom 555 4321000 100 montgomery st 10th floor summary current fulltime professional graduating fall masters degree business administration people friendly outstanding communication organizational skills experience working people ages academic workplace accomplishments prove dedication excellence years schooling combined full time work testament work ethic personal drive passion food beverage industry experience corporate fine dining restaurants peoplefacing numberfacing facets looking lifelong career combines skills passions business food service hospitality industry skills sixsigma green belt crm office management software workforce management inventory budget management strong communication skills customer relations billinginvoicing budgetary planning scheduling calendar management event coordination compelling leadership skills education training grwestern carolina university 122021 mba university north carolina asheville 100 montgomery st 10th floor 052019 bachelor science business management minor economics minor political science chancellors list 4years merit scholar recipient certifications sixsigma green belt servsafe manager certified sales force project management certificate leadership motivated curious experience holistic industries program management office manager royal oak mi 122018 current part project management team 5 year contract national center environmental information supporting data stewardship within organization manage labor invoices staffing aspects subcontract management plan subcontracting companies analyze maintain records financial records including cost proposals contract modifications subcontract labor charges schedule coordinate meetings technical companies employees government contractors coordinate lead onboarding activities new employees including paperwork training sops write monthly status reports technical implementation plans contracting officers government task managers provided weekly monthly presentation government stakeholders funders contract progress invoicingreporting statuses cultivated impactful relationships government customers drove business development delivering product knowledge product owners developers federal contractor teams quail ridge country club server bartender boca raton fl 062020 current helped chef storm rhum bar open brand new restaurant august 2020 part opening staff keep track opentable reservations coordinateplan staffing gm kitchen appropriate serve assist parties gourmet food drink selection delivery providing excellent customer experience without server support creating serving cocktails servers bar including mixing drinks pouring wine serving guests bar communication front back house including bar kitchen emphasis knowledge large selection top shelf liquors wines draft beers chipotle mexican grill assistant general manager city state 122013 122018 responsible staff coordination including training food safety scheduling oversees labor management profit loss accounting inventory tracking daily monthly cash balance sheet auditing placed biweekly food inventory orders oversaw correct restocking procedures maintain operational output manager food quality oversight entire restaurant including flow produce waste effectively mastered use erestaurant altametrics taleo management began crew member promoted twice within company fostered performanceoriented environment focused promoting team collaboration personal accountability longterm business success controlled business inventory keep numbers beneath targets expert oversight usage monitoring storm rhum bar bistro server city state 012017 052019 serve assist parties gourmet food drink selection delivery providing excellent customer experience without server support creating serving cocktails servers bar including mixing drinks pouring wine serving guests bar communication front back house including bar kitchen emphasis knowledge large selection top shelf liquors wines draft beers\",\n          \"linda lewis construction manager summary professional construction manager 5 years experience expert managing multimillion dollar projects conception fruition skills emotional intelligence entrepreneurial qualities strong communication leadership risk management negotiation personal organization project recovery task management business case writing meetings management sense humour adaptability education background blackwell university college science class 2021 certificates concrete transportation construction coating inspector risk management professional pmirmp work experience construction project manager blackwell lab april 2020 2022 ensured construction activities move according predetermined schedule scheduled subcontractors consultants vendors critical path ensure timely completion construction manager fincor construction may 2016 apr 2020 monitored progress construction activities regular basis hold regular status meetings subteams presided preconstruction meeting subcontractor assisted budgeting bidding award subcontracts 1234567890 123 anywhere st city state country 12345 helloresumesbotcom linkedin resumesbot check building construction resume examples\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12233,\n        \"samples\": [\n          \"  floor resumesampleexamplecom summary proven skills receiving moving stocking merchandise efficient accurate hardworking team player knowledgeable grocery stocking rotation procedures comfortable lifting pounds least years worth customer service enjoy working customers adding team add benefits hard working enjoy working skills product displays safety methods merchandise stocking able lift lbs clean work area customer service experience apparel stalker pactiv bedford  adaptive team player dependable reliable product organization boxing labeling merchandise restock followed proper stock rotation procedures minimize obsolescence remove date items sales floor received incoming product deliveries relocated storage shelves coolers bins lifted materials varied weights regular basis moved rebuilt shelves racks displays increase visibility merchandise faced products shelves displays meet company policies stocked shelves new merchandise checked outdated damaged items recorded merchandise moves correct product counts checked quantities stocking reports replenished items maintain stock availability updated products new pricing temporary promotion signs increase sales arranged product shelves following merchandising guidelines broke discarded empty packaging boxes stocking products maintained merchandise presentation stocking rotating merchandise received stocked merchandise organized maintained backroom following company safety cleaning operating procedures processing freight preparing backroom incoming freight stocked pallets throughout shift keep warehouse clean maintained organized used accurate efficient storage procedures shelve new merchandise returned items arranged organized merchandise supplies identified shrink damages maintain work environment attached price labels products updated sales floor signs moved items central storage room shelves customer purchase prepared products adding tags readying pallets restocking carried duties within fastpaced retail environment providing organized stocking methods plans verified product numbers scanners manual tracking replenished inventory focus addressing customer needs assisted backroom organization replacing products fixtures equipment stored items orderly accessible manner warehouse tool rooms supply rooms areas updated pricing sales signs promote merchandise case packer  co boca raton fl arranged items pallets according size weight marked labeled containers accurate shipping information prevent delays reviewed packing slips documentation properly box requested items shipment assembled cartons crates containers prepare shipping operated hand trucks pallet jacks forklifts move materials assisted line changeovers returned raw material stock adhered product safety hygiene guidelines health regulatory compliance reported safety workflow concerns leadership investigation weighed counted items distribution conform company standards verified labels appropriate order numbers item specifications notified team leader product discrepancies equipment malfunctions trained new employees regarding warehouse procedures standards inspected orders accuracy carefully reviewing containers products packaging labeling measured weighed counted products materials sealed containers materials using glues fasteners tape cleaned containers supplies work areas using cleaning solutions loaded materials products package processing equipment staged loaded pallets hand truck fork truck pickup remove completed defective products materials placing conveyors loading docks salvage  foods tennessee city state cut cleaned product meet standards company making sure every product safe consumption salvaging parts product reduce waiste product handled delegated tasks customer service associate publix city state supported customer service goals enhanced relations friendly knowledgeable positive communication delivered fast friendly knowledgeable service routine questions service complaints met exceeded productivity targets handling every interaction topnotch customer service provided outstanding service new longstanding customers attending closely concerns developing solutions organized prioritized tasks activities worked within strict timeframes deadlines trained new hires products services best practices protocols reduce process gaps education training high school diploma island coast high school cape coral fl\",\n          \" senior financial specialist personal info email christophereliasgmailcom phone linkedin  skills  excel  analysis mathematical skills communication skills database management time management organization financial statement analysis techniques business valuation highly professional senior financial specialist years experience field managed projects portfolios budgets 200m performed detailed financial analysis companies throughout career saved aggregate tens billions dollars clients seeking use proven skills enhance profitability daybreak llc work history present education senior financial specialist pacific bay financial corporation san francisco ca created implemented financial planning method saved client 220m direct costs annually trained new team financial specialists reducing companys turnover rate incorporated new expense management procedure generated 8m monthly savings across company departments generated financial reports state california managed significant companys tax filings clearing outstanding tax liabilities financial specialist  san francisco ca streamlined client databases led increase process efficiency performed analysis new projects generated upwards 40m revenue created detailed budgeting plans lowering costs production comanaged 150m company portfolio grew value year junior financial specialist grow finance irvine ca managed companys database system clients performed financial analysis clients increasing client companys performance 1296 conducted presented detailed cash flow analysis master finance uc irvine merage school business ca member university finance club took extracurricular data analytics courses gpa score courses certificates certification finance quantitative modeling analysts university pennsylvania financial reporting certificate university illinois certified financial planner certified financial planner board standards\",\n          \"  floor resumesampleexamplecom summary reliable team member strong communication skills ability adjust work various environments handles modifications requests feedback colleagues clients vendors efficient manner understanding needs parties involved flexible adapting work style fit different projects juggling multiple assignments within tight deadlines skills contract development management verbal written communication sales marketing experience operations manager current  management built strong operational teams meet process production demands implemented policies standard operating procedures managed quality customer service logistics addressed customer concerns suitable solutions delivered positive customer experiences implementing effective quality assurance practices oversaw financial management budget management accounting payroll activities enforced federal state local company rules safety operations collected customer fees managed refunds provided complete sales documentation generated performance labor strategies compete new existing markets resources teacher lead teacher assistant teacher hollyfrontier corporation millington tn built strengthened positive relationships students parents teaching staff prepared maintained classroom environments appropriate student learning physical social emotional development managed student behavior classroom establishing enforcing rules procedures engaged students boosted understanding material using focused instructional strategies handson activities graded student papers assignments track student progression utilized behavior management skills foster environment conducive student learning scheduled held parentteacher conferences keep parents uptodate childrens academic performance designed individualized curricula academically underachieving students district sales manager  state supervises new store openings assigned area works recruiting developing motivating teams deliver companys vision assessed locations individual team performances analyzing data trends determine best methods improve sales coached sales associates product specifications sales incentives selling techniques significantly increasing customer satisfaction ratings worked diligently dealers management teams forge lasting relationships assist solving unique business problems rectified billing issues quickly maintain client satisfaction assessed locations individual team performances analyzing data trends determine best methods improve sales results set clear objectives helped team members develop plans achieve quotas trained mentored sales team members customer relations customer service product placement created maintained sales environment support business objectives special education assistant carriage house school city state developed designs specifically market certain clients aligning designs clients tastes preferences researched analyzed market trends inform design decisions implemented costsaving initiatives reduce overhead costs helped teachers develop lesson plans curriculums classroom instruction facilitated classroom discussions promote creative thinking various subjects led small group activities discussions help promote learning collaborated teacher adapting curriculum meet individual student goals helped prepare distribute materials activities communicated important school information announcements students parents helped maintain positive learning atmosphere classroom supported successful inclusion children special needs developed presentations visual aids promote student engagement lectures developed maintained consistent positive relationships children facilitated safe effective transitions activities developed strategies meet needs students variety disabilities established enforced rules behavior procedures maintain order among students taught socially acceptable behavior employing behavior modification positive reinforcement techniques facilitated small group instruction enhance student learning contributed creation implementation individualized education plans implemented individualized behavioral plans students disabilities encouraged explored activities help students improve gross fine motor skills prepared classrooms variety materials resources children explore manipulate use learning activities imaginative play taught socially acceptable behavior employing techniques behavior modification positive reinforcement provided disabled students assistive devices supportive technology help accessing facilities restrooms provided assistance students special needs education training high school diploma daviess county high school owensboro ky skills team leadership verbal written communication customer relationship management staff management time management managing multiple tasks administration reporting income expense management alanna wuest fire fluff boutique communications radio tv university southern indiana evansville\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df = pd.read_csv('/content/clean_resume_dataset.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the lengths of all documents\n",
        "text_lengths = df['clean_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Get statistics about the lengths\n",
        "print(f\"Max Length: {text_lengths.max()}\")\n",
        "print(f\"Min Length: {text_lengths.min()}\")\n",
        "print(f\"Average Length: {text_lengths.mean()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-UM54dUkNbk",
        "outputId": "4f676938-388b-445d-d0c4-6d0034fba238"
      },
      "id": "O-UM54dUkNbk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Length: 6146\n",
            "Min Length: 4\n",
            "Average Length: 439.7765436131983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5807b5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5807b5d",
        "outputId": "8836d852-2515-490c-8f9d-d699428dd9f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique job roles: 43\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "\n",
        "# --- Step 3: Encode Labels ---\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['Category'])\n",
        "num_labels = len(label_encoder.classes_)\n",
        "print(f\"Number of unique job roles: {num_labels}\")\n",
        "label_encoder.classes_\n",
        "\n",
        "# --- Step 4: Train-Test Split (First Split) ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['clean_text'],\n",
        "    df['label'],\n",
        "    test_size=0.1,  # 10% for testing\n",
        "    random_state=42,\n",
        "    stratify=df['label']  # Ensures class balance\n",
        ")\n",
        "\n",
        "# --- Step 5: Tokenize Text for BERT ---\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "num_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BASE MODEL"
      ],
      "metadata": {
        "id": "tQnXwXFcqM3x"
      },
      "id": "tQnXwXFcqM3x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f6b08b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43f6b08b",
        "outputId": "05b40856-21d8-4eed-90e0-200827f76092"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Training fold 1...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [02:53<00:00,  2.65it/s, loss=3.4]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.22it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6468\n",
            "Val Loss: 3.1894\n",
            "Val Accuracy: 0.3077\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [02:52<00:00,  2.67it/s, loss=1.45]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.25it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.4556\n",
            "Val Loss: 1.6782\n",
            "Val Accuracy: 0.7335\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [02:52<00:00,  2.67it/s, loss=4.2]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.3433\n",
            "Val Loss: 1.1101\n",
            "Val Accuracy: 0.7708\n",
            "\n",
            "============================================================\n",
            "Fold 1 Results:\n",
            "  Accuracy:  0.7708\n",
            "  Precision: 0.7493\n",
            "  Recall:    0.7445\n",
            "  F1-Score:  0.7398\n",
            "  Time:      742.18s\n",
            "============================================================\n",
            "\n",
            "Training fold 2...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:53<00:00,  2.65it/s, loss=2.95]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.6127\n",
            "Val Loss: 3.0554\n",
            "Val Accuracy: 0.4816\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:51<00:00,  2.68it/s, loss=1.49]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.3660\n",
            "Val Loss: 1.6196\n",
            "Val Accuracy: 0.7343\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:51<00:00,  2.68it/s, loss=0.316]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3001\n",
            "Val Loss: 1.0744\n",
            "Val Accuracy: 0.7860\n",
            "\n",
            "============================================================\n",
            "Fold 2 Results:\n",
            "  Accuracy:  0.7860\n",
            "  Precision: 0.8086\n",
            "  Recall:    0.7654\n",
            "  F1-Score:  0.7738\n",
            "  Time:      739.98s\n",
            "============================================================\n",
            "\n",
            "Training fold 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:53<00:00,  2.65it/s, loss=3.05]\n",
            "Validation: 100%|██████████| 230/230 [00:28<00:00,  8.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.6239\n",
            "Val Loss: 3.0847\n",
            "Val Accuracy: 0.4179\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:51<00:00,  2.68it/s, loss=1.37]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.3830\n",
            "Val Loss: 1.6015\n",
            "Val Accuracy: 0.7465\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:51<00:00,  2.68it/s, loss=1.88]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3168\n",
            "Val Loss: 1.0527\n",
            "Val Accuracy: 0.7852\n",
            "\n",
            "============================================================\n",
            "Fold 3 Results:\n",
            "  Accuracy:  0.7852\n",
            "  Precision: 0.7567\n",
            "  Recall:    0.7538\n",
            "  F1-Score:  0.7500\n",
            "  Time:      740.21s\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FINAL K-FOLD CROSS-VALIDATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Accuracy:  0.7807 ± 0.0070\n",
            "Precision: 0.7715 ± 0.0264\n",
            "Recall:    0.7545 ± 0.0086\n",
            "F1-Score:  0.7545 ± 0.0142\n",
            "\n",
            "Total Time: 2222.36s\n",
            "Average Time per Fold: 740.79s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Configuration ---\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "LEARNING_RATE = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# --- Step 6: Cross-Validation Setup ---\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 1\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "fold_results = []\n",
        "\n",
        "# --- Step 7: K-Fold Cross-Validation ---\n",
        "for train_index, val_index in kfold.split(X_train):\n",
        "    print(f\"\\nTraining fold {fold_no}...\")\n",
        "\n",
        "    # Track time for this fold\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "        list(X_train_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_encodings = tokenizer(\n",
        "        list(X_val_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(y_train_fold.values)\n",
        "    )\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(y_val_fold.values)\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # --- Load Model (PyTorch version) ---\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    # --- Optimizer & Scheduler ---\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=1e-6,\n",
        "        end_factor=1.0,\n",
        "        total_iters=total_steps\n",
        "    )\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_pbar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "        for batch in train_pbar:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc='Validation'):\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                total_val_loss += outputs.loss.item()\n",
        "\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            final_preds.extend(preds.cpu().numpy())\n",
        "            final_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(final_labels, final_preds)\n",
        "    report = classification_report(\n",
        "        final_labels,\n",
        "        final_preds,\n",
        "        target_names=label_encoder.classes_,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    fold_result = {\n",
        "        'fold': fold_no,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': report['macro avg']['precision'],\n",
        "        'recall': report['macro avg']['recall'],\n",
        "        'f1_score': report['macro avg']['f1-score'],\n",
        "        'time': time.time() - start_time,\n",
        "        'report': report\n",
        "    }\n",
        "\n",
        "    fold_results.append(fold_result)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold_no} Results:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {report['macro avg']['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"  Time:      {fold_result['time']:.2f}s\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# --- Final Results ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL K-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "accuracies = [f['accuracy'] for f in fold_results]\n",
        "precisions = [f['precision'] for f in fold_results]\n",
        "recalls = [f['recall'] for f in fold_results]\n",
        "f1_scores = [f['f1_score'] for f in fold_results]\n",
        "\n",
        "print(f\"\\nAccuracy:  {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "\n",
        "print(f\"\\nTotal Time: {sum(f['time'] for f in fold_results):.2f}s\")\n",
        "print(f\"Average Time per Fold: {np.mean([f['time'] for f in fold_results]):.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation studies: Batch size 16 -> Batch size 32"
      ],
      "metadata": {
        "id": "U9MlpkPHp68c"
      },
      "id": "U9MlpkPHp68c"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 32\n",
        "MAX_LENGTH = 128\n",
        "LEARNING_RATE = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# --- Step 6: Cross-Validation Setup ---\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 1\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "fold_results = []\n",
        "\n",
        "# --- Step 7: K-Fold Cross-Validation ---\n",
        "for train_index, val_index in kfold.split(X_train):\n",
        "    print(f\"\\nTraining fold {fold_no}...\")\n",
        "\n",
        "    # Track time for this fold\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "        list(X_train_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_encodings = tokenizer(\n",
        "        list(X_val_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(y_train_fold.values)\n",
        "    )\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(y_val_fold.values)\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # --- Load Model (PyTorch version) ---\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    # --- Optimizer & Scheduler ---\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=1e-6,\n",
        "        end_factor=1.0,\n",
        "        total_iters=total_steps\n",
        "    )\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_pbar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "        for batch in train_pbar:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc='Validation'):\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                total_val_loss += outputs.loss.item()\n",
        "\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            final_preds.extend(preds.cpu().numpy())\n",
        "            final_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(final_labels, final_preds)\n",
        "    report = classification_report(\n",
        "        final_labels,\n",
        "        final_preds,\n",
        "        target_names=label_encoder.classes_,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    fold_result = {\n",
        "        'fold': fold_no,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': report['macro avg']['precision'],\n",
        "        'recall': report['macro avg']['recall'],\n",
        "        'f1_score': report['macro avg']['f1-score'],\n",
        "        'time': time.time() - start_time,\n",
        "        'report': report\n",
        "    }\n",
        "\n",
        "    fold_results.append(fold_result)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold_no} Results:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {report['macro avg']['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"  Time:      {fold_result['time']:.2f}s\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# --- Final Results ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL K-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "accuracies = [f['accuracy'] for f in fold_results]\n",
        "precisions = [f['precision'] for f in fold_results]\n",
        "recalls = [f['recall'] for f in fold_results]\n",
        "f1_scores = [f['f1_score'] for f in fold_results]\n",
        "\n",
        "print(f\"\\nAccuracy:  {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "\n",
        "print(f\"\\nTotal Time: {sum(f['time'] for f in fold_results):.2f}s\")\n",
        "print(f\"Average Time per Fold: {np.mean([f['time'] for f in fold_results]):.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Aq3230gp2mz",
        "outputId": "528e83af-e0a4-4b53-fb23-c7eeb95a0b6a"
      },
      "id": "_Aq3230gp2mz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training fold 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:42<00:00,  1.42it/s, loss=3.39]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.7242\n",
            "Val Loss: 3.4705\n",
            "Val Accuracy: 0.1865\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:40<00:00,  1.43it/s, loss=2.27]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.8870\n",
            "Val Loss: 2.1551\n",
            "Val Accuracy: 0.6638\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:40<00:00,  1.44it/s, loss=0.809]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6913\n",
            "Val Loss: 1.2881\n",
            "Val Accuracy: 0.7563\n",
            "\n",
            "============================================================\n",
            "Fold 1 Results:\n",
            "  Accuracy:  0.7563\n",
            "  Precision: 0.7406\n",
            "  Recall:    0.7295\n",
            "  F1-Score:  0.7201\n",
            "  Time:      697.87s\n",
            "============================================================\n",
            "\n",
            "Training fold 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:42<00:00,  1.42it/s, loss=3.54]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.6635\n",
            "Val Loss: 3.3257\n",
            "Val Accuracy: 0.2287\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:40<00:00,  1.43it/s, loss=2.49]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.8128\n",
            "Val Loss: 2.0977\n",
            "Val Accuracy: 0.6545\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:40<00:00,  1.44it/s, loss=1.1]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.7126\n",
            "Val Loss: 1.2839\n",
            "Val Accuracy: 0.7550\n",
            "\n",
            "============================================================\n",
            "Fold 2 Results:\n",
            "  Accuracy:  0.7550\n",
            "  Precision: 0.7283\n",
            "  Recall:    0.7172\n",
            "  F1-Score:  0.7093\n",
            "  Time:      698.41s\n",
            "============================================================\n",
            "\n",
            "Training fold 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:41<00:00,  1.42it/s, loss=3.5]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.6868\n",
            "Val Loss: 3.3840\n",
            "Val Accuracy: 0.2238\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:40<00:00,  1.43it/s, loss=2.02]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.8155\n",
            "Val Loss: 2.0786\n",
            "Val Accuracy: 0.6654\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 230/230 [02:40<00:00,  1.43it/s, loss=1.12]\n",
            "Validation: 100%|██████████| 115/115 [00:25<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6991\n",
            "Val Loss: 1.2592\n",
            "Val Accuracy: 0.7686\n",
            "\n",
            "============================================================\n",
            "Fold 3 Results:\n",
            "  Accuracy:  0.7686\n",
            "  Precision: 0.7500\n",
            "  Recall:    0.7353\n",
            "  F1-Score:  0.7332\n",
            "  Time:      696.67s\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FINAL K-FOLD CROSS-VALIDATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Accuracy:  0.7600 ± 0.0061\n",
            "Precision: 0.7396 ± 0.0089\n",
            "Recall:    0.7273 ± 0.0075\n",
            "F1-Score:  0.7209 ± 0.0098\n",
            "\n",
            "Total Time: 2092.95s\n",
            "Average Time per Fold: 697.65s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation Studies: Max length 128 -> Max length 256"
      ],
      "metadata": {
        "id": "-UeC4lO0qTGf"
      },
      "id": "-UeC4lO0qTGf"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 256\n",
        "LEARNING_RATE = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# --- Step 6: Cross-Validation Setup ---\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 1\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "fold_results = []\n",
        "\n",
        "# --- Step 7: K-Fold Cross-Validation ---\n",
        "for train_index, val_index in kfold.split(X_train):\n",
        "    print(f\"\\nTraining fold {fold_no}...\")\n",
        "\n",
        "    # Track time for this fold\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "        list(X_train_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_encodings = tokenizer(\n",
        "        list(X_val_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(y_train_fold.values)\n",
        "    )\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(y_val_fold.values)\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # --- Load Model (PyTorch version) ---\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    # --- Optimizer & Scheduler ---\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=1e-6,\n",
        "        end_factor=1.0,\n",
        "        total_iters=total_steps\n",
        "    )\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_pbar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "        for batch in train_pbar:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc='Validation'):\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                total_val_loss += outputs.loss.item()\n",
        "\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            final_preds.extend(preds.cpu().numpy())\n",
        "            final_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(final_labels, final_preds)\n",
        "    report = classification_report(\n",
        "        final_labels,\n",
        "        final_preds,\n",
        "        target_names=label_encoder.classes_,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    fold_result = {\n",
        "        'fold': fold_no,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': report['macro avg']['precision'],\n",
        "        'recall': report['macro avg']['recall'],\n",
        "        'f1_score': report['macro avg']['f1-score'],\n",
        "        'time': time.time() - start_time,\n",
        "        'report': report\n",
        "    }\n",
        "\n",
        "    fold_results.append(fold_result)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold_no} Results:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {report['macro avg']['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"  Time:      {fold_result['time']:.2f}s\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# --- Final Results ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL K-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "accuracies = [f['accuracy'] for f in fold_results]\n",
        "precisions = [f['precision'] for f in fold_results]\n",
        "recalls = [f['recall'] for f in fold_results]\n",
        "f1_scores = [f['f1_score'] for f in fold_results]\n",
        "\n",
        "print(f\"\\nAccuracy:  {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "\n",
        "print(f\"\\nTotal Time: {sum(f['time'] for f in fold_results):.2f}s\")\n",
        "print(f\"Average Time per Fold: {np.mean([f['time'] for f in fold_results]):.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D62qSBOGqTPM",
        "outputId": "aeb4d94f-4651-4334-d8c7-b762d52e3b66"
      },
      "id": "D62qSBOGqTPM",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training fold 1...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [05:15<00:00,  1.46it/s, loss=3.01]\n",
            "Validation: 100%|██████████| 230/230 [00:50<00:00,  4.54it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6241\n",
            "Val Loss: 3.2501\n",
            "Val Accuracy: 0.1835\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [05:23<00:00,  1.42it/s, loss=0.943]\n",
            "Validation: 100%|██████████| 230/230 [00:50<00:00,  4.57it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.4713\n",
            "Val Loss: 1.5911\n",
            "Val Accuracy: 0.7370\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [05:24<00:00,  1.42it/s, loss=0.283]\n",
            "Validation: 100%|██████████| 230/230 [00:50<00:00,  4.56it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.1675\n",
            "Val Loss: 0.8600\n",
            "Val Accuracy: 0.8334\n",
            "\n",
            "============================================================\n",
            "Fold 1 Results:\n",
            "  Accuracy:  0.8334\n",
            "  Precision: 0.8479\n",
            "  Recall:    0.8186\n",
            "  F1-Score:  0.8182\n",
            "  Time:      1276.76s\n",
            "============================================================\n",
            "\n",
            "Training fold 2...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [05:24<00:00,  1.42it/s, loss=3.22]\n",
            "Validation: 100%|██████████| 230/230 [00:50<00:00,  4.57it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6438\n",
            "Val Loss: 3.2126\n",
            "Val Accuracy: 0.2559\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [05:23<00:00,  1.42it/s, loss=1.98]\n",
            "Validation: 100%|██████████| 230/230 [00:50<00:00,  4.57it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.4125\n",
            "Val Loss: 1.5074\n",
            "Val Accuracy: 0.7672\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [05:23<00:00,  1.42it/s, loss=2.3]\n",
            "Validation: 100%|██████████| 230/230 [00:50<00:00,  4.56it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.1463\n",
            "Val Loss: 0.8401\n",
            "Val Accuracy: 0.8339\n",
            "\n",
            "============================================================\n",
            "Fold 2 Results:\n",
            "  Accuracy:  0.8339\n",
            "  Precision: 0.8373\n",
            "  Recall:    0.8173\n",
            "  F1-Score:  0.8180\n",
            "  Time:      1283.37s\n",
            "============================================================\n",
            "\n",
            "Training fold 3...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [05:11<00:00,  1.48it/s, loss=2.93]\n",
            "Validation: 100%|██████████| 230/230 [00:49<00:00,  4.61it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6207\n",
            "Val Loss: 3.0754\n",
            "Val Accuracy: 0.3757\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [05:24<00:00,  1.42it/s, loss=2.25]\n",
            "Validation: 100%|██████████| 230/230 [00:50<00:00,  4.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.3552\n",
            "Val Loss: 1.5027\n",
            "Val Accuracy: 0.7620\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [05:23<00:00,  1.42it/s, loss=0.25]\n",
            "Validation: 100%|██████████| 230/230 [00:50<00:00,  4.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1427\n",
            "Val Loss: 0.8135\n",
            "Val Accuracy: 0.8424\n",
            "\n",
            "============================================================\n",
            "Fold 3 Results:\n",
            "  Accuracy:  0.8424\n",
            "  Precision: 0.8508\n",
            "  Recall:    0.8262\n",
            "  F1-Score:  0.8304\n",
            "  Time:      1273.18s\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FINAL K-FOLD CROSS-VALIDATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Accuracy:  0.8366 ± 0.0041\n",
            "Precision: 0.8453 ± 0.0058\n",
            "Recall:    0.8207 ± 0.0039\n",
            "F1-Score:  0.8222 ± 0.0058\n",
            "\n",
            "Total Time: 3833.32s\n",
            "Average Time per Fold: 1277.77s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation Studies: Learning rate 2e-5 -> Learning Rate 5e-5"
      ],
      "metadata": {
        "id": "XwwwSr0zqjJ_"
      },
      "id": "XwwwSr0zqjJ_"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "LEARNING_RATE = 5e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# --- Step 6: Cross-Validation Setup ---\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 1\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "fold_results = []\n",
        "\n",
        "# --- Step 7: K-Fold Cross-Validation ---\n",
        "for train_index, val_index in kfold.split(X_train):\n",
        "    print(f\"\\nTraining fold {fold_no}...\")\n",
        "\n",
        "    # Track time for this fold\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "        list(X_train_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_encodings = tokenizer(\n",
        "        list(X_val_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(y_train_fold.values)\n",
        "    )\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(y_val_fold.values)\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # --- Load Model (PyTorch version) ---\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    # --- Optimizer & Scheduler ---\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=1e-6,\n",
        "        end_factor=1.0,\n",
        "        total_iters=total_steps\n",
        "    )\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_pbar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "        for batch in train_pbar:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc='Validation'):\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                total_val_loss += outputs.loss.item()\n",
        "\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            final_preds.extend(preds.cpu().numpy())\n",
        "            final_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(final_labels, final_preds)\n",
        "    report = classification_report(\n",
        "        final_labels,\n",
        "        final_preds,\n",
        "        target_names=label_encoder.classes_,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    fold_result = {\n",
        "        'fold': fold_no,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': report['macro avg']['precision'],\n",
        "        'recall': report['macro avg']['recall'],\n",
        "        'f1_score': report['macro avg']['f1-score'],\n",
        "        'time': time.time() - start_time,\n",
        "        'report': report\n",
        "    }\n",
        "\n",
        "    fold_results.append(fold_result)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold_no} Results:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {report['macro avg']['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"  Time:      {fold_result['time']:.2f}s\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# --- Final Results ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL K-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "accuracies = [f['accuracy'] for f in fold_results]\n",
        "precisions = [f['precision'] for f in fold_results]\n",
        "recalls = [f['recall'] for f in fold_results]\n",
        "f1_scores = [f['f1_score'] for f in fold_results]\n",
        "\n",
        "print(f\"\\nAccuracy:  {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "\n",
        "print(f\"\\nTotal Time: {sum(f['time'] for f in fold_results):.2f}s\")\n",
        "print(f\"Average Time per Fold: {np.mean([f['time'] for f in fold_results]):.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW3G0Oj6qjl-",
        "outputId": "e704ddc9-fadb-49f6-d823-ec0c5cf9207d"
      },
      "id": "HW3G0Oj6qjl-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training fold 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:46<00:00,  2.77it/s, loss=1.8]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.3001\n",
            "Val Loss: 2.2047\n",
            "Val Accuracy: 0.6523\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:46<00:00,  2.77it/s, loss=0.987]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5433\n",
            "Val Loss: 1.1410\n",
            "Val Accuracy: 0.7550\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:45<00:00,  2.77it/s, loss=0.129]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9194\n",
            "Val Loss: 0.9649\n",
            "Val Accuracy: 0.7822\n",
            "\n",
            "============================================================\n",
            "Fold 1 Results:\n",
            "  Accuracy:  0.7822\n",
            "  Precision: 0.7806\n",
            "  Recall:    0.7578\n",
            "  F1-Score:  0.7588\n",
            "  Time:      726.89s\n",
            "============================================================\n",
            "\n",
            "Training fold 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:46<00:00,  2.77it/s, loss=2.78]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.3144\n",
            "Val Loss: 2.2674\n",
            "Val Accuracy: 0.6352\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:45<00:00,  2.77it/s, loss=0.738]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5808\n",
            "Val Loss: 1.1154\n",
            "Val Accuracy: 0.7678\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:45<00:00,  2.78it/s, loss=2.69]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9172\n",
            "Val Loss: 0.9377\n",
            "Val Accuracy: 0.7849\n",
            "\n",
            "============================================================\n",
            "Fold 2 Results:\n",
            "  Accuracy:  0.7849\n",
            "  Precision: 0.8055\n",
            "  Recall:    0.7633\n",
            "  F1-Score:  0.7735\n",
            "  Time:      713.41s\n",
            "============================================================\n",
            "\n",
            "Training fold 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:46<00:00,  2.77it/s, loss=1.4]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.3801\n",
            "Val Loss: 2.2929\n",
            "Val Accuracy: 0.6104\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:45<00:00,  2.77it/s, loss=0.238]\n",
            "Validation: 100%|██████████| 230/230 [00:27<00:00,  8.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.5846\n",
            "Val Loss: 1.0699\n",
            "Val Accuracy: 0.7735\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:46<00:00,  2.77it/s, loss=0.338]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9182\n",
            "Val Loss: 0.9038\n",
            "Val Accuracy: 0.7947\n",
            "\n",
            "============================================================\n",
            "Fold 3 Results:\n",
            "  Accuracy:  0.7947\n",
            "  Precision: 0.8074\n",
            "  Recall:    0.7863\n",
            "  F1-Score:  0.7887\n",
            "  Time:      714.94s\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "FINAL K-FOLD CROSS-VALIDATION RESULTS\n",
            "============================================================\n",
            "\n",
            "Accuracy:  0.7873 ± 0.0054\n",
            "Precision: 0.7979 ± 0.0122\n",
            "Recall:    0.7691 ± 0.0123\n",
            "F1-Score:  0.7737 ± 0.0122\n",
            "\n",
            "Total Time: 2155.24s\n",
            "Average Time per Fold: 718.41s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation Studies: Weight decay 0.01 -> Weight decay 0.1"
      ],
      "metadata": {
        "id": "IWDXLJzkqyS0"
      },
      "id": "IWDXLJzkqyS0"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "EPOCHS = 3\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "LEARNING_RATE = 2e-5\n",
        "WEIGHT_DECAY = 0.1\n",
        "\n",
        "# --- Step 6: Cross-Validation Setup ---\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 1\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "fold_results = []\n",
        "\n",
        "# --- Step 7: K-Fold Cross-Validation ---\n",
        "for train_index, val_index in kfold.split(X_train):\n",
        "    print(f\"\\nTraining fold {fold_no}...\")\n",
        "\n",
        "    # Track time for this fold\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "        list(X_train_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_encodings = tokenizer(\n",
        "        list(X_val_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(y_train_fold.values)\n",
        "    )\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(y_val_fold.values)\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # --- Load Model (PyTorch version) ---\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    # --- Optimizer & Scheduler ---\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=1e-6,\n",
        "        end_factor=1.0,\n",
        "        total_iters=total_steps\n",
        "    )\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_pbar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "        for batch in train_pbar:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc='Validation'):\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                total_val_loss += outputs.loss.item()\n",
        "\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            final_preds.extend(preds.cpu().numpy())\n",
        "            final_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(final_labels, final_preds)\n",
        "    report = classification_report(\n",
        "        final_labels,\n",
        "        final_preds,\n",
        "        target_names=label_encoder.classes_,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    fold_result = {\n",
        "        'fold': fold_no,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': report['macro avg']['precision'],\n",
        "        'recall': report['macro avg']['recall'],\n",
        "        'f1_score': report['macro avg']['f1-score'],\n",
        "        'time': time.time() - start_time,\n",
        "        'report': report\n",
        "    }\n",
        "\n",
        "    fold_results.append(fold_result)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold_no} Results:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {report['macro avg']['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"  Time:      {fold_result['time']:.2f}s\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# --- Final Results ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL K-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "accuracies = [f['accuracy'] for f in fold_results]\n",
        "precisions = [f['precision'] for f in fold_results]\n",
        "recalls = [f['recall'] for f in fold_results]\n",
        "f1_scores = [f['f1_score'] for f in fold_results]\n",
        "\n",
        "print(f\"\\nAccuracy:  {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "\n",
        "print(f\"\\nTotal Time: {sum(f['time'] for f in fold_results):.2f}s\")\n",
        "print(f\"Average Time per Fold: {np.mean([f['time'] for f in fold_results]):.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bfJ2ycgqycd",
        "outputId": "37b7edbb-646e-49e9-e680-e54bafc415bd"
      },
      "id": "8bfJ2ycgqycd",
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training fold 1...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [02:46<00:00,  2.77it/s, loss=3.57]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.59it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.6321\n",
            "Val Loss: 3.1723\n",
            "Val Accuracy: 0.3488\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [02:45<00:00,  2.77it/s, loss=1.13]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.57it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.4392\n",
            "Val Loss: 1.6497\n",
            "Val Accuracy: 0.7310\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 460/460 [02:45<00:00,  2.77it/s, loss=0.554]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.53it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.3201\n",
            "Val Loss: 1.0786\n",
            "Val Accuracy: 0.7727\n",
            "\n",
            "============================================================\n",
            "Fold 1 Results:\n",
            "  Accuracy:  0.7727\n",
            "  Precision: 0.7922\n",
            "  Recall:    0.7610\n",
            "  F1-Score:  0.7628\n",
            "  Time:      714.69s\n",
            "============================================================\n",
            "\n",
            "Training fold 2...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:46<00:00,  2.77it/s, loss=3.75]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 3.6482\n",
            "Val Loss: 3.1350\n",
            "Val Accuracy: 0.4005\n",
            "\n",
            "Epoch 2/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:45<00:00,  2.77it/s, loss=1.74]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.4146\n",
            "Val Loss: 1.6082\n",
            "Val Accuracy: 0.7365\n",
            "\n",
            "Epoch 3/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:45<00:00,  2.77it/s, loss=0.641]\n",
            "Validation: 100%|██████████| 230/230 [00:26<00:00,  8.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3055\n",
            "Val Loss: 1.0592\n",
            "Val Accuracy: 0.7746\n",
            "\n",
            "============================================================\n",
            "Fold 2 Results:\n",
            "  Accuracy:  0.7746\n",
            "  Precision: 0.7949\n",
            "  Recall:    0.7434\n",
            "  F1-Score:  0.7435\n",
            "  Time:      713.75s\n",
            "============================================================\n",
            "\n",
            "Training fold 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 460/460 [02:46<00:00,  2.77it/s, loss=3.5]\n",
            "Validation:  37%|███▋      | 86/230 [00:10<00:16,  8.56it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ablation Studies: Epoch 3 -> Epoch 4"
      ],
      "metadata": {
        "id": "p4yNiCtgq700"
      },
      "id": "p4yNiCtgq700"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration ---\n",
        "EPOCHS = 4\n",
        "BATCH_SIZE = 16\n",
        "MAX_LENGTH = 128\n",
        "LEARNING_RATE = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "\n",
        "# --- Step 6: Cross-Validation Setup ---\n",
        "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "fold_no = 1\n",
        "accuracies = []\n",
        "precisions = []\n",
        "recalls = []\n",
        "f1_scores = []\n",
        "fold_results = []\n",
        "\n",
        "# --- Step 7: K-Fold Cross-Validation ---\n",
        "for train_index, val_index in kfold.split(X_train):\n",
        "    print(f\"\\nTraining fold {fold_no}...\")\n",
        "\n",
        "    # Track time for this fold\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Split data\n",
        "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
        "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "    train_encodings = tokenizer(\n",
        "        list(X_train_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    val_encodings = tokenizer(\n",
        "        list(X_val_fold),\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        train_encodings['input_ids'],\n",
        "        train_encodings['attention_mask'],\n",
        "        torch.tensor(y_train_fold.values)\n",
        "    )\n",
        "\n",
        "    val_dataset = torch.utils.data.TensorDataset(\n",
        "        val_encodings['input_ids'],\n",
        "        val_encodings['attention_mask'],\n",
        "        torch.tensor(y_val_fold.values)\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # --- Load Model (PyTorch version) ---\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        'bert-base-uncased',\n",
        "        num_labels=num_labels\n",
        "    ).to(device)\n",
        "\n",
        "    # --- Optimizer & Scheduler ---\n",
        "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    total_steps = len(train_loader) * EPOCHS\n",
        "    scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "        optimizer,\n",
        "        start_factor=1e-6,\n",
        "        end_factor=1.0,\n",
        "        total_iters=total_steps\n",
        "    )\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f'\\nEpoch {epoch + 1}/{EPOCHS}')\n",
        "\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        train_pbar = tqdm(train_loader, desc='Training')\n",
        "\n",
        "        for batch in train_pbar:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc='Validation'):\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "                outputs = model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    labels=labels\n",
        "                )\n",
        "\n",
        "                total_val_loss += outputs.loss.item()\n",
        "\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        print(f'Train Loss: {avg_train_loss:.4f}')\n",
        "        print(f'Val Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Val Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "    # --- Final Evaluation ---\n",
        "    model.eval()\n",
        "    final_preds = []\n",
        "    final_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            final_preds.extend(preds.cpu().numpy())\n",
        "            final_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(final_labels, final_preds)\n",
        "    report = classification_report(\n",
        "        final_labels,\n",
        "        final_preds,\n",
        "        target_names=label_encoder.classes_,\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    fold_result = {\n",
        "        'fold': fold_no,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': report['macro avg']['precision'],\n",
        "        'recall': report['macro avg']['recall'],\n",
        "        'f1_score': report['macro avg']['f1-score'],\n",
        "        'time': time.time() - start_time,\n",
        "        'report': report\n",
        "    }\n",
        "\n",
        "    fold_results.append(fold_result)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold_no} Results:\")\n",
        "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {report['macro avg']['precision']:.4f}\")\n",
        "    print(f\"  Recall:    {report['macro avg']['recall']:.4f}\")\n",
        "    print(f\"  F1-Score:  {report['macro avg']['f1-score']:.4f}\")\n",
        "    print(f\"  Time:      {fold_result['time']:.2f}s\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    fold_no += 1\n",
        "\n",
        "    # Clean up memory\n",
        "    del model\n",
        "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "# --- Final Results ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"FINAL K-FOLD CROSS-VALIDATION RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "accuracies = [f['accuracy'] for f in fold_results]\n",
        "precisions = [f['precision'] for f in fold_results]\n",
        "recalls = [f['recall'] for f in fold_results]\n",
        "f1_scores = [f['f1_score'] for f in fold_results]\n",
        "\n",
        "print(f\"\\nAccuracy:  {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
        "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
        "print(f\"Recall:    {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
        "print(f\"F1-Score:  {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
        "\n",
        "print(f\"\\nTotal Time: {sum(f['time'] for f in fold_results):.2f}s\")\n",
        "print(f\"Average Time per Fold: {np.mean([f['time'] for f in fold_results]):.2f}s\")\n"
      ],
      "metadata": {
        "id": "pxrYjZhMq78l"
      },
      "id": "pxrYjZhMq78l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c93001",
      "metadata": {
        "id": "08c93001",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02778895-b3f4-45c3-ac63-6eb5b7868450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Epoch 1/3 on Full Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 689/689 [04:18<00:00,  2.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.0730\n",
            "\n",
            "Training Epoch 2/3 on Full Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 689/689 [04:20<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9004\n",
            "\n",
            "Training Epoch 3/3 on Full Data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 689/689 [04:19<00:00,  2.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.6674\n",
            "Test Accuracy: 0.8098\n",
            "\n",
            "Classification Report on Test Set:\n",
            "{'0': {'precision': 0.8333333333333334, 'recall': 0.8823529411764706, 'f1-score': 0.8571428571428571, 'support': 34.0}, '1': {'precision': 0.7297297297297297, 'recall': 0.9310344827586207, 'f1-score': 0.8181818181818182, 'support': 29.0}, '2': {'precision': 0.8421052631578947, 'recall': 0.6956521739130435, 'f1-score': 0.7619047619047619, 'support': 23.0}, '3': {'precision': 0.6388888888888888, 'recall': 0.71875, 'f1-score': 0.6764705882352942, 'support': 32.0}, '4': {'precision': 0.782608695652174, 'recall': 0.6, 'f1-score': 0.6792452830188679, 'support': 30.0}, '5': {'precision': 0.8076923076923077, 'recall': 0.84, 'f1-score': 0.8235294117647058, 'support': 25.0}, '6': {'precision': 0.7391304347826086, 'recall': 0.5666666666666667, 'f1-score': 0.6415094339622641, 'support': 30.0}, '7': {'precision': 0.8857142857142857, 'recall': 0.9393939393939394, 'f1-score': 0.9117647058823529, 'support': 33.0}, '8': {'precision': 0.875, 'recall': 0.9655172413793104, 'f1-score': 0.9180327868852459, 'support': 29.0}, '9': {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 5.0}, '10': {'precision': 0.7567567567567568, 'recall': 0.8484848484848485, 'f1-score': 0.8, 'support': 33.0}, '11': {'precision': 0.8421052631578947, 'recall': 1.0, 'f1-score': 0.9142857142857143, 'support': 32.0}, '12': {'precision': 1.0, 'recall': 0.5, 'f1-score': 0.6666666666666666, 'support': 20.0}, '13': {'precision': 0.9310344827586207, 'recall': 0.8709677419354839, 'f1-score': 0.9, 'support': 31.0}, '14': {'precision': 0.8333333333333334, 'recall': 0.7352941176470589, 'f1-score': 0.78125, 'support': 34.0}, '15': {'precision': 0.8214285714285714, 'recall': 0.8214285714285714, 'f1-score': 0.8214285714285714, 'support': 28.0}, '16': {'precision': 0.6206896551724138, 'recall': 0.72, 'f1-score': 0.6666666666666666, 'support': 25.0}, '17': {'precision': 0.6956521739130435, 'recall': 0.64, 'f1-score': 0.6666666666666666, 'support': 25.0}, '18': {'precision': 0.8888888888888888, 'recall': 0.96, 'f1-score': 0.9230769230769231, 'support': 25.0}, '19': {'precision': 0.7674418604651163, 'recall': 0.9705882352941176, 'f1-score': 0.8571428571428571, 'support': 34.0}, '20': {'precision': 0.8148148148148148, 'recall': 0.7333333333333333, 'f1-score': 0.7719298245614035, 'support': 30.0}, '21': {'precision': 0.75, 'recall': 0.8076923076923077, 'f1-score': 0.7777777777777778, 'support': 26.0}, '22': {'precision': 0.825, 'recall': 0.8461538461538461, 'f1-score': 0.8354430379746836, 'support': 39.0}, '23': {'precision': 0.8571428571428571, 'recall': 0.972972972972973, 'f1-score': 0.9113924050632911, 'support': 37.0}, '24': {'precision': 0.8275862068965517, 'recall': 0.7272727272727273, 'f1-score': 0.7741935483870968, 'support': 33.0}, '25': {'precision': 0.9230769230769231, 'recall': 0.8571428571428571, 'f1-score': 0.8888888888888888, 'support': 14.0}, '26': {'precision': 0.7714285714285715, 'recall': 0.9, 'f1-score': 0.8307692307692308, 'support': 30.0}, '27': {'precision': 0.8571428571428571, 'recall': 0.967741935483871, 'f1-score': 0.9090909090909091, 'support': 31.0}, '28': {'precision': 0.6857142857142857, 'recall': 0.9230769230769231, 'f1-score': 0.7868852459016393, 'support': 26.0}, '29': {'precision': 0.8709677419354839, 'recall': 0.84375, 'f1-score': 0.8571428571428571, 'support': 32.0}, '30': {'precision': 0.6111111111111112, 'recall': 0.6875, 'f1-score': 0.6470588235294118, 'support': 32.0}, '31': {'precision': 0.9411764705882353, 'recall': 0.9411764705882353, 'f1-score': 0.9411764705882353, 'support': 34.0}, '32': {'precision': 0.96, 'recall': 0.7741935483870968, 'f1-score': 0.8571428571428571, 'support': 31.0}, '33': {'precision': 0.9642857142857143, 'recall': 0.8181818181818182, 'f1-score': 0.8852459016393442, 'support': 33.0}, '34': {'precision': 0.7368421052631579, 'recall': 0.56, 'f1-score': 0.6363636363636364, 'support': 25.0}, '35': {'precision': 0.84375, 'recall': 0.8709677419354839, 'f1-score': 0.8571428571428571, 'support': 31.0}, '36': {'precision': 0.8888888888888888, 'recall': 0.7272727272727273, 'f1-score': 0.8, 'support': 22.0}, '37': {'precision': 0.75, 'recall': 0.5454545454545454, 'f1-score': 0.631578947368421, 'support': 11.0}, '38': {'precision': 0.9642857142857143, 'recall': 0.9310344827586207, 'f1-score': 0.9473684210526315, 'support': 29.0}, '39': {'precision': 0.8235294117647058, 'recall': 0.56, 'f1-score': 0.6666666666666666, 'support': 25.0}, '40': {'precision': 0.7575757575757576, 'recall': 0.7142857142857143, 'f1-score': 0.7352941176470589, 'support': 35.0}, '41': {'precision': 0.7777777777777778, 'recall': 0.8235294117647058, 'f1-score': 0.8, 'support': 34.0}, '42': {'precision': 0.7333333333333333, 'recall': 0.7857142857142857, 'f1-score': 0.7586206896551724, 'support': 28.0}, 'accuracy': 0.809795918367347, 'macro avg': {'precision': 0.8192317318105257, 'recall': 0.8028971769662836, 'f1-score': 0.8044683448201466, 'support': 1225.0}, 'weighted avg': {'precision': 0.8156263006809633, 'recall': 0.809795918367347, 'f1-score': 0.8069173205380993, 'support': 1225.0}}\n"
          ]
        }
      ],
      "source": [
        "# After k-fold cross-validation, train a final model on the entire training data\n",
        "# Train the final model on the entire training set (all data used in k-folds)\n",
        "final_model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=num_labels\n",
        ").to(device)\n",
        "\n",
        "final_optimizer = AdamW(final_model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
        "\n",
        "# Tokenize the entire training data\n",
        "final_train_encodings = tokenizer(\n",
        "    list(X_train),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "final_train_dataset = torch.utils.data.TensorDataset(\n",
        "    final_train_encodings['input_ids'],\n",
        "    final_train_encodings['attention_mask'],\n",
        "    torch.tensor(y_train.values)\n",
        ")\n",
        "\n",
        "final_train_loader = torch.utils.data.DataLoader(final_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Train the model on the entire training data\n",
        "final_model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'\\nTraining Epoch {epoch + 1}/{EPOCHS} on Full Data')\n",
        "\n",
        "    total_train_loss = 0\n",
        "    train_pbar = tqdm(final_train_loader, desc='Training')\n",
        "\n",
        "    for batch in train_pbar:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        final_model.zero_grad()\n",
        "\n",
        "        outputs = final_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), 1.0)\n",
        "        final_optimizer.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(final_train_loader)\n",
        "    print(f'Train Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "# Final evaluation on the test set using the model trained on all the data\n",
        "final_model.eval()\n",
        "\n",
        "# Tokenize the test set\n",
        "final_test_encodings = tokenizer(\n",
        "    list(X_test),\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "final_test_labels = torch.tensor(y_test.values).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = final_model(\n",
        "        input_ids=final_test_encodings['input_ids'].to(device),\n",
        "        attention_mask=final_test_encodings['attention_mask'].to(device)\n",
        "    )\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=1)  # Get the predicted class labels\n",
        "\n",
        "# Evaluate the model's performance on the test set\n",
        "test_accuracy = accuracy_score(final_test_labels.cpu(), predictions.cpu())  # Move to CPU for accuracy calculation\n",
        "\n",
        "# Print test accuracy and classification report\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Optional: print the classification report\n",
        "report = classification_report(final_test_labels.cpu(), predictions.cpu(), output_dict=True, zero_division=0)\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text you want to classify\n",
        "custom_text = [\"Dynamic and results-driven Human Resources professional with [X] years of experience in recruitment, employee relations, and organizational development. Proven expertise in managing end-to-end recruitment processes, onboarding, performance management, and implementing HR policies to ensure a positive work culture. Strong communication and problem-solving skills with a focus on enhancing employee engagement and supporting business objectives.\"]\n",
        "\n",
        "# Tokenize the custom text (ensure to handle the padding and truncation as needed)\n",
        "custom_encodings = tokenizer(\n",
        "    custom_text,\n",
        "    truncation=True,\n",
        "    padding=True,\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "# Move the tokenized input to the correct device\n",
        "input_ids = custom_encodings['input_ids'].to(device)\n",
        "attention_mask = custom_encodings['attention_mask'].to(device)\n",
        "\n",
        "# Make prediction with the trained model\n",
        "final_model.eval()  # Set model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = final_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    logits = outputs.logits\n",
        "    prediction = torch.argmax(logits, dim=1).item()  # Get the predicted class index\n",
        "\n",
        "# Map the prediction back to the corresponding class label\n",
        "predicted_label = label_encoder.classes_[prediction]\n",
        "\n",
        "# Print the predicted label\n",
        "print(f\"Predicted Label: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RAEfBVHd7MF",
        "outputId": "0832047c-1f8d-45fb-993e-a2179f1bbc81"
      },
      "id": "5RAEfBVHd7MF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Label: Human Resources\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}